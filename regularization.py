# -*- coding: utf-8 -*-
"""Regularization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pDOt_j9kVyYfCAe2ZtT6PDgBJqPcRkkH
"""

#import lib we need .

import tensorflow as tf
from tensorflow import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.regularizers import l2

from sklearn.preprocessing import StandardScaler


import numpy as np
import matplotlib.pyplot as plt

import random

#fix all possible random
seed=0
np.random.seed(seed)
random.seed(seed)
tf.random.set_seed(seed)

# Generate the data
x_train_ = np.linspace(-100, 100, 5000).reshape(-1, 1)



# Define the function for y_train
def my_fun(x):
    return x**3+3*x+5*x**9
# Compute y_train using the unnormalized x_train_
y_train_= my_fun(x_train)

x_s = StandardScaler()
y_s=StandardScaler()

x_train= x_s.fit_transform(x_train_)
y_train= y_s.fit_transform(y_train_)



# Check for NaN and Inf in y_train
print(np.isnan(y_train).sum())  # Should be 0 if no NaNs
print(np.isinf(y_train).sum())  # Should be 0 if no Infs

lambda_=.1   #regularization strenght


# Build the model
Module = Sequential([
    Dense(128, activation='relu', input_shape=(x_train.shape[1],),kernel_regularizer=l2(lambda_)),
    Dense(64, activation='relu',kernel_regularizer=l2(lambda_)),
    Dense(32, activation='relu',kernel_regularizer=l2(lambda_)),
    Dense(8, activation='relu',kernel_regularizer=l2(lambda_)),
    Dense(1, activation='linear')
])

# Compile the model
Module.compile(
    optimizer='adam',
    loss='mean_squared_error',  # Appropriate for regression
    metrics=['mean_absolute_error']  # Suitable metric for regression
)

# Fit the model
history = Module.fit(
    x_train, y_train,
    epochs=100,
    batch_size=256,
    validation_split=0.2  # 10% of the data is used for validation
)

# Plot the results
plt.figure(figsize=(12, 5))

# MAE plot
plt.subplot(2, 2, 1)
plt.plot(history.history['mean_absolute_error'], label='Training MAE', color='r')
#plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE', color='b')
plt.legend()
plt.grid(True)
plt.xlabel("Epochs")
plt.ylabel("Mean Absolute Error")

plt.subplot(2, 2, 2)
plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE', color='b')
plt.legend()
plt.grid(True)
plt.xlabel("Epochs")
plt.ylabel("Mean Absolute Error")


# Loss plot
plt.subplot(2, 2, 3)
plt.plot(history.history['loss'], label='Training Loss', color='r')
#plt.plot(history.history['val_loss'], label='Validation Loss', color='b')
plt.legend()
plt.grid(True)
plt.xlabel("Epochs")
plt.ylabel("Mean Squared Error Loss")

plt.subplot(2, 2, 4)
plt.plot(history.history['val_loss'], label='Validation Loss', color='b')
plt.legend()
plt.grid(True)
plt.xlabel("Epochs")
plt.ylabel("Mean Squared Error Loss")


plt.tight_layout()
plt.show()

x_plot = np.linspace(-100, 100, 1000).reshape(-1, 1)
x_plot_scaled = x_s.transform(x_plot)

y_pred_scaled = Module.predict(x_plot_scaled)
y_pred = y_s.inverse_transform(y_pred_scaled)

plt.figure(figsize=(10, 4))
plt.plot(x_plot, my_fun(x_plot), label='True function', alpha=0.6)
plt.plot(x_plot, y_pred, label='Model prediction', alpha=0.8)
plt.legend()
plt.title("True vs. Predicted")
plt.grid(True)
plt.show()

Module.summary()

